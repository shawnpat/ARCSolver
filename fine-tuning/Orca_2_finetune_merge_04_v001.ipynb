{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e011afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q sentencepiece\n",
    "# %pip install -q protobuf\n",
    "# %pip install -q -U bitsandbytes\n",
    "# %pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "# %pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "# %pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "# %pip install -q scipy\n",
    "# %pip install -q -U trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cd23a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, PeftConfig, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8832169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import re\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "class CustomSFTTrainer(SFTTrainer):\n",
    "    def __init__(self, tokenizer, *args, **kwargs):\n",
    "        super(CustomSFTTrainer, self).__init__(*args, **kwargs)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        inputs_copy = inputs.copy()\n",
    "        labels = inputs_copy.pop(\"labels\")\n",
    "        outputs = model(**inputs_copy)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Compute the loss using the superclass's compute_loss method\n",
    "        super_loss, _ = super().compute_loss(model, inputs, return_outputs=True)\n",
    "        print(\"\\npreloss:\", super_loss.item())\n",
    "        loss = super_loss\n",
    "        \n",
    "        wrong_proportion = 1\n",
    "        \n",
    "        labels_decoded = decoded = self.tokenizer.batch_decode(labels.tolist())\n",
    "        labels_result = labels_decoded[0]\n",
    "        \n",
    "        try:\n",
    "            target_match = re.search(r\"Test_1_Output=\\[\\[.*?\\]\\]\", labels_result)\n",
    "            target_array_string = target_match.group(0).replace(\"Test_1_Output=\", \"\")\n",
    "            target_array = np.array(ast.literal_eval(target_array_string))\n",
    "            \n",
    "            # Apply softmax to logits to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            # Get the predicted tokens (highest probability)\n",
    "            _, predictions = torch.max(probs, dim=-1)\n",
    "            \n",
    "            decoded = self.tokenizer.batch_decode(predictions.tolist())\n",
    "            predictions_result = decoded[0]\n",
    "            match1 = re.search(r\"Test_1_Output=\\[\\[.*?\\]\\]\", predictions_result)\n",
    "            predicted_array_string = \"\"\n",
    "            if match1:\n",
    "                predicted_array_string = match1.group(0).replace(\"Test_1_Output=\", \"\")\n",
    "                print(\"matched Test_1_Output=[[*?]]\")\n",
    "            else:\n",
    "                loss *= 2\n",
    "                print(\"Did not match Test_1_Output=[[*?]]..predicted\")\n",
    "                print(predictions_result)\n",
    "\n",
    "            if predicted_array_string != \"\":\n",
    "                try:\n",
    "                    predicted_array = np.array(ast.literal_eval(predicted_array_string))\n",
    "                    if predicted_array.shape == target_array.shape:\n",
    "                        print(\"correct shape\")\n",
    "                        loss *= loss\n",
    "                        equal_elements_count = np.sum(np.equal(target_array, predicted_array))\n",
    "                        total_elements_count = target_array.size + 1\n",
    "                        wrong_elements_count = total_elements_count - equal_elements_count\n",
    "                        wrong_proportion = wrong_elements_count / total_elements_count\n",
    "                        loss *= wrong_proportion * wrong_proportion\n",
    "                        are_equal = np.array_equal(predicted_array, target_array)\n",
    "                        if are_equal:\n",
    "                            print(\"are equal!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "                            print(\"target\")\n",
    "                            print(target_array)\n",
    "                            print(\"predicted\")\n",
    "                            print(predicted_array)\n",
    "                            loss *= loss\n",
    "                    else:\n",
    "                        print(\"incorrect shape\")\n",
    "                        print(\"target\")\n",
    "                        print(target_array)\n",
    "                        print(\"predicted\")\n",
    "                        print(predicted_array)\n",
    "                except:\n",
    "                    print(\"failed to make predicted array\")\n",
    "                    print(predicted_array_string)\n",
    "        except:\n",
    "            print(\"failed to make target array\")\n",
    "            print(labels_result)\n",
    "        \n",
    "        print(\"wp:\", wrong_proportion, \"  loss:\", loss.item())\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "851eae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    user_message = data_point[\"instruction\"]\n",
    "    assistant_message = data_point[\"output\"]\n",
    "    text = f\"<|im_start|>user\\n{user_message}<|im_end|>\\n<|im_start|>assistant\\n{assistant_message}<|im_end|>\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fbe72e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"merged_models/Orca_2_finetune_04_merge\"\n",
    "new_adapter_name = \"Orca-2-13b-4bit-finetune-on-ARC-15\"\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa6858e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abdb8df8adc0424c8c0bd9d4a3f8db19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11240 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1be18213b00c44f5979b100fd395a496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/72504 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf594e190f3142f89cf75b900c2b5a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the data using pandas\n",
    "data_file = \"data/core_ARC_puzzles.json\"\n",
    "df = pd.read_json(data_file)\n",
    "# Convert the pandas dataframe to a dataset\n",
    "dataset3 = Dataset.from_pandas(df)\n",
    "\n",
    "# Load the data using pandas\n",
    "data_file = \"data/ARC_augmented_training_puzzles.json\"\n",
    "df = pd.read_json(data_file)\n",
    "# Convert the pandas dataframe to a dataset\n",
    "dataset1 = Dataset.from_pandas(df)\n",
    "\n",
    "# Load the data using pandas\n",
    "data_file = \"data/ARC_augmented_evaluation_puzzles.json\"\n",
    "df = pd.read_json(data_file)\n",
    "# Convert the pandas dataframe to a dataset\n",
    "dataset2 = Dataset.from_pandas(df)\n",
    "\n",
    "# Generate the \"prompt\" column for each dataset\n",
    "dataset1 = dataset1.map(lambda data_point: {\"prompt\": generate_prompt(data_point)})\n",
    "dataset2 = dataset2.map(lambda data_point: {\"prompt\": generate_prompt(data_point)})\n",
    "dataset3 = dataset3.map(lambda data_point: {\"prompt\": generate_prompt(data_point)})\n",
    "\n",
    "# # Assuming dataset1 and dataset2 are instances of Dataset from the `datasets` library\n",
    "dataset = concatenate_datasets([dataset1, dataset2, dataset3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88f1c385",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load base model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=device_map,\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model = prepare_model_for_kbit_training(model)  \n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name, add_eos_token=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"  # Fix weird overflow issue with fp16 training\n",
    "\n",
    "    per_device_train_batch_size = 1\n",
    "    gradient_accumulation_steps = 1\n",
    "    max_seq_length = 5000\n",
    "\n",
    "    output_dir = \"results/\" + new_adapter_name\n",
    "\n",
    "    steps_per_epoch = len(dataset) // (\n",
    "        per_device_train_batch_size * gradient_accumulation_steps\n",
    "    )\n",
    "    print(\"Steps:\", steps_per_epoch)\n",
    "\n",
    "    # Set training parameters\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        max_steps=steps_per_epoch,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "#         weight_decay=0.01,\n",
    "        save_strategy=\"steps\",\n",
    "        evaluation_strategy=\"no\",\n",
    "        save_steps=1000,\n",
    "        logging_steps=1,\n",
    "        learning_rate=5e-6,\n",
    "        fp16=True,\n",
    "        warmup_steps=0.03,\n",
    "        group_by_length=True,\n",
    "        gradient_checkpointing=True,\n",
    "    )\n",
    "\n",
    "    trainer = CustomSFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"prompt\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save trained model\n",
    "    adapter = \"adapters/\" + new_adapter_name\n",
    "    trainer.model.save_pretrained(adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4175f06a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972cdca5370646868329e835801701e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 93744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993e07d1ce374358bd8c4226743abf20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/93744 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "preloss: 0.6165382266044617\n",
      "Did not match Test_1_Output=[[*?]]..predicted\n",
      "<|im_start|>user\n",
      "Train_1_Input=[[0,5,2,6,6,6,6,7,9,7,7,7,5,2,6,1,1,1,1,1,1,1,1,1],[5,1,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,9,1,1],[1,1,1,1,1,1,1,1,8,1,1,1],[1,1,1,1,1,1,1,1,1,1,0,1],[1,1,1,1,1,1,9,1,9,9,5,9],[1,1,1,1,1,1,1,1,1,1,9,1],[1,1,1,1,1,1,1,1,1,1,8,1],[1,1,1,9,9,9,1,9,9,9,0,0],[9,1,9,1,1,1,1,9,9,1,1,1],[1,1,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,9,1,1,1,1,1,0],[1,1,1,1,1,1,1,1,1,1,3,3],[1,1,1,1,1,1,1,1,0,0,0,0],[1,1,0,9,1,1,1,0,3,9,9,9],[1,9,0,9,9,1,1,1,1,1,1,1],[1,1,9,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,1,9],[1,1,1,1,1,1,1,1,1,1,1,1],[1,1,9,1,1,1,1,1,1,1,1,1],[ Train_1_Output=[[1,0,0,0,0,0,1,0,0,2,0,0],[0,0,0,0,0,0,0,4,9,0,0,0],[0,0,0,0,0,0,1,6,0,0,0,0],[0,0,0,0,0,8,9,8,2,8,0,0],[0,5,1,4,9,8,0,4,0,0,0,0],[0,0,1,2,0,0,0,0,0,0,0,0],[0,9,9,8,2,0,0,0,0,0,0,0],[0,5,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,1,4,9],[0,0,0,0,0,0,0,0,7,1,0,8],[0,0,0,0,0,0,0,0,0,9,8,2],[0,0,0,0,0,0,0,0,3,0,0,0],[ Train_2_Input=[[1,1,1,1,1,1,1,1,1,1,1,4,1,1,9,1,1,1,1,1,9,1],[8,1,1,9,1,1,1,1,1,1,9],[1,1,1,1,1,1,1,1,1,1,3],[1,5,8,2,9,9,1,5,3,5,9],[5,9,9,9,9,9,1,9,5,5,3],[3,1,0,1,3,9,9,5,4,4,0],[3,3,1,0,9,1,7,5,0,0,0],[7,1,4,9,0,0,0,9,3,9,3],[5,1,2,9,9,0,9,8,8,9,1],[5,9,8,2,9,9,8,0,5,1,1],[3,0,9,1,9,1,9,9,9,9,1],[7,1,1,9,9,9,9,1,1,9,1],[0,3,9,9,9,1,9,1,1,1,9],[0,9,9,1,1,1,1,1,9,1,9],[ Train_2_Output=[[0,0,0,0,0,0,0,0,0,0,0],[0,3,0,6,7,0,0,0,0,0,0],[0,1,4,9,0,0,0,0,0,0,0],[7,1,2,8,0,0,0,0,0,0,0],[8,9,8,2,0,0,0,0,0,0,0],[3,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0],[0,0,9,6,0,0,0,0,0,0,0],[5,1,4,9,0,0,0,0,0,0,0],[7,1,0,8,0,0,0,0,0,0,0],[8,9,8,2,0,0,0,0,9,6,6],[0,0,0,0,0,0,0,1,4,9,0],[0,0,0,0,0,0,7,1,0,8,0],[8,0,0,0,0,0,8,9,8,2,0],[0,0,0,0,0,0,0,0,0,0,0],[ Test_3_Input=[[1,1,1,1,1,1,1,1,1,1,8,3,9,9,1,1,1,9,1,9],[3,9,9,1,1,1,9,9,9,3],[2,1,1,1,1,1,1,4,9,1],[7,9,1,4,9,9,9,4,9,9],[7,9,9,2,9,9,9,8,8,9],[5,3,9,8,2,9,9,0,8,8],[1,8,9,1,9,8,1,4,9,1],[9,1,9,9,9,9,1,4,9,9],[7,9,9,9,9,1,9,8,2,9],[ Train_3_Output=[[0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,9,1,0],[0,0,0,9,6,0,1,4,9,0],[0,5,1,4,9,7,1,8,8,0],[0,7,1,0,8,0,9,8,2,0],[0,8,9,8,2,7,1,9,6,0],[0,0,0,0,0,0,1,4,9,0],[0,0,0,0,0,7,1,0,8,0],[0,0,0,0,0,8,9,8,2,0]] Test_1_Input=[[8,0,1,0,1,1,1,1,4,9,3,1],[7,3,1,4,3,0,1,4,4,9,5,5,6,9,4,4,1,9,4,4,6,4],[5,5,0,4,4,4,0,0,4,0,0,1,4,0,0,0,0],[0,0,4,4,9,0,1,4,4,0,3,0,0,8,0,4,0],[0,9,0,4,9,0,9,4,4,4,0,0,0,0,0,9,0],[0,3,8,4,9,0,0,1,4,0,9,9,0,0,9,4,2],[0,4,8,8,4,9,0,5,4,8,9,5,3,0,1,4,3],[1,1,2,8,4,8,8,4,4,8,1,4,8,8,8,8,8],[1,1,4,4,8,2,0,1,2,8,4,4,8,1,8,8,1],[5,1,6,6,4,1,1,4,4,4,4,4,1,3,4,4,3],[5,0,1,1,1,4,1,1,4,4,0,4,3,3,8,0,0],[0,5,4,4,4,5,9,4,4,3,3,1,3,1,3,8,3],[0,1,1,3,7,1,1,4,3,6,6,8,9,1,4,9,0],[5,9,1,9,9,8,5,1,6,9,9,0,1,1,8,8,2],[5,9,9,4,9,7,0,1,9,6,9,3,8,9,8,2,0],[5,9,9,7,5,8,8,8,1,1,1,1,3,1,1,4,1],[3,1,3,6,1,1,1,0,1,1,1,1,1,1,1,1,1],[3,0,0,8,0,0,1,1,6,0,0,1,1,0,8,6,3],[0,0,8,1,4,0,1,4,3,1,2,1,1,4,9,0,3],[0,0,0,4,5,1,4,1,8,3,1,8,1,1,7,8,3],[0,2,2,2,9,0,0,1,8,3,1,3,1,8,2,3,3],[0,1,1,1,1,1,9,1,8,3,1,3,3,3,8,4,3],[3,6,6,1,4,1,1,1,3,1,3,4,2,3,9,8,2],[3,4,1,1,1,3,1,1,2,2,2,3,1,0,1,1,3],[3,0,1,1,9,0,4,1,8,6,1,0,1,8,8,2,3],[0,3,9,4,4,1,0,6,4,1,0,9,0,8,8,8,0],[0,1,1,2,9,8,1,3,1,4,3,1,0,3,1,1,3],[0,1,1,1,5,8,1,1,8,1,0,0,0,1,3,8,0],[0,6,1,1,1,1,1,6,8,8,1,0,1,0,8,3,2],[<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Test_1_Output=[[0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,9,6,0,0,0,0,0,0,0,0,0,0,0],[0,0,7,1,4,9,0,0,0,0,0,0,0,0,0,0,0],[0,0,7,1,0,8,0,0,0,0,0,0,0,0,0,0,0],[0,0,8,9,8,2,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,9,6,0],[0,0,0,0,0,0,0,0,0,0,0,0,5,1,4,9,0],[0,0,0,0,0,0,0,0,0,0,0,0,7,1,0,8,0],[0,0,0,0,0,0,0,0,0,0,0,0,8,9,8,2,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,9,6,0],[0],[0,0,0,0,0,0,0,0,0,0,0,5,1,4,9,0,0],[0,0,0,0,0,0,0,0,0,0,0,7,1,0,8,0,0],[0,0,0,0,0,0,0,0,0,0,0,8,9,8,2,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[<|im_end|>\n",
      "\n",
      "wp: 1   loss: 1.2330764532089233\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='93744' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    2/93744 : < :, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "preloss: 0.38128259778022766\n",
      "matched Test_1_Output=[[*?]]\n",
      "correct shape\n",
      "wp: 0.030470914127423823   loss: 0.00013497860345523804\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor []], which is output 0 of MulBackward0, is at version 2; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 59\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomSFTTrainer(\n\u001b[1;32m     49\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     50\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_arguments,\n\u001b[1;32m     56\u001b[0m )\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Save trained model\u001b[39;00m\n\u001b[1;32m     62\u001b[0m adapter \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madapters/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m new_adapter_name\n",
      "File \u001b[0;32m~/mambaforge/envs/tensorml/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:280\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 280\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m~/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/trainer.py:1530\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/trainer.py:1844\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1841\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1844\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1847\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1850\u001b[0m ):\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1852\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/mambaforge/envs/tensorml/lib/python3.11/site-packages/transformers/trainer.py:2710\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2708\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2709\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2710\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2712\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/mambaforge/envs/tensorml/lib/python3.11/site-packages/accelerate/accelerator.py:1903\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1902\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1903\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1904\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1905\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/tensorml/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/tensorml/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor []], which is output 0 of MulBackward0, is at version 2; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
