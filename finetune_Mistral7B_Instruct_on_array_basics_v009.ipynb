{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i0H8-b_PpSTV","executionInfo":{"status":"ok","timestamp":1698792435572,"user_tz":240,"elapsed":2843,"user":{"displayName":"Shawn McInerney","userId":"16229430297181086922"}},"outputId":"1148a8aa-6697-4afd-e66c-80e15e532ffb"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1698792435572,"user":{"displayName":"Shawn McInerney","userId":"16229430297181086922"},"user_tz":240},"id":"LXaLSUf-xK3f"},"outputs":[],"source":["gdrive = \"/content/drive/\""]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1698792435572,"user":{"displayName":"Shawn McInerney","userId":"16229430297181086922"},"user_tz":240},"id":"lXXHLDn0ww6m"},"outputs":[],"source":["import os\n","import shutil\n","\n","pretrained_model_name = \"Mistral-7B-Instruct\"\n","output_dir = gdrive + \"MyDrive/\" + \"outputs_\" + pretrained_model_name+ \"_finetuned_on_10000_array_basics\"\n","finetuned_model_folder = gdrive + \"MyDrive/\" + pretrained_model_name + \"_finetuned_on_10000_array_basics\"\n","\n","if os.path.exists(output_dir):\n","    shutil.rmtree(output_dir)\n","\n","if os.path.exists(finetuned_model_folder):\n","    shutil.rmtree(finetuned_model_folder)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":61215,"status":"ok","timestamp":1698792496786,"user":{"displayName":"Shawn McInerney","userId":"16229430297181086922"},"user_tz":240},"id":"3nNWXXc7ol1n","outputId":"6c0b05a5-c148-483d-f503-53acc64d0b9d"},"outputs":[{"output_type":"stream","name":"stdout","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["%pip install -q -U bitsandbytes\n","%pip install -q -U git+https://github.com/huggingface/transformers.git\n","%pip install -q -U git+https://github.com/huggingface/peft.git\n","%pip install -q -U git+https://github.com/huggingface/accelerate.git\n","%pip install -q datasets\n","%pip install -q scipy"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":6575,"status":"ok","timestamp":1698792503359,"user":{"displayName":"Shawn McInerney","userId":"16229430297181086922"},"user_tz":240},"id":"kvvLg99Opw5R"},"outputs":[],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, DataCollatorForLanguageModeling\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16\n",")"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":102,"referenced_widgets":["5fee1338b240490c8005655b5889e095","c39bfa793443401cb29f3d6e27d39d0a","6764c8e69733490b84a50eb46af596ca","956c0c8d8f8d40d2bd184e9ccff04197","cf46938c0c5e415b96f865e23c930d52","16453c65225344a5bd070fcb61356976","019b195413ea4d9899f572939f75df0c","e8e0d381885c43f6a3101f41e0f47fdf","7aec764934ac45c894d58ae89d8189bf","8f89ee9eccd84926bb13f8413d93233f","8b4f48571db74c938603f7c3aaa8580c"]},"executionInfo":{"elapsed":19774,"status":"ok","timestamp":1698792523131,"user":{"displayName":"Shawn McInerney","userId":"16229430297181086922"},"user_tz":240},"id":"7St-hFLNmS2v","outputId":"9c4696bc-6689-4fbb-e273-838cbd57274e"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fee1338b240490c8005655b5889e095"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["'\\nThe following warnings can be ignored. I think they are caused by the fact that the model\\nis loaded in a single GPU and the parameters are offloaded to the CPU:\\n\\nWARNING:root:Some parameters are on the meta device device because they were offloaded to the .\\nWARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu/disk.\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}],"source":["pretrained_model = \"mistralai/Mistral-7B-Instruct-v0.1\"\n","model = AutoModelForCausalLM.from_pretrained(pretrained_model, quantization_config=bnb_config, device_map={\"\":0})\n","tokenizer = AutoTokenizer.from_pretrained(pretrained_model, add_eos_token=True)\n","\n","'''\n","The following warnings can be ignored. I think they are caused by the fact that the model\n","is loaded in a single GPU and the parameters are offloaded to the CPU:\n","\n","WARNING:root:Some parameters are on the meta device device because they were offloaded to the .\n","WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu/disk.\n","'''"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":807,"status":"ok","timestamp":1698792523929,"user":{"displayName":"Shawn McInerney","userId":"16229430297181086922"},"user_tz":240},"id":"NAP-jYBjrwUc"},"outputs":[],"source":["import pandas as pd\n","import datasets\n","datasets.disable_progress_bar()\n","from datasets import Dataset\n","\n","# Load the data using pandas\n","data_file = gdrive + \"MyDrive/\" + \"ARCSolver_core_knowledge_on_basic_arrays_10000.json\"\n","df = pd.read_json(data_file)\n","\n","# Convert the pandas dataframe to a dataset\n","data = Dataset.from_pandas(df)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":1346,"status":"ok","timestamp":1698792525274,"user":{"displayName":"Shawn McInerney","userId":"16229430297181086922"},"user_tz":240},"id":"Mzyp0VROww6n"},"outputs":[],"source":["def generate_prompt(data_point):\n","    text = '<s>[INST] ' + data_point[\"instruction\"] + ' [/INST] ' + str(data_point[\"output\"]) + '</s>'\n","    return text\n","\n","# add the \"prompt\" column in the dataset\n","text_column = [generate_prompt(data_point) for data_point in data]\n","data = data.add_column(\"prompt\", text_column)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1698792525274,"user":{"displayName":"Shawn McInerney","userId":"16229430297181086922"},"user_tz":240},"id":"3TKCLTOVDR1x"},"outputs":[],"source":["data = data.train_test_split(test_size=0.2)\n","train_data = data[\"train\"]\n","test_data = data[\"test\"]"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1698792525274,"user":{"displayName":"Shawn McInerney","userId":"16229430297181086922"},"user_tz":240},"id":"NMELsVV6q2my"},"outputs":[],"source":["from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n","\n","model.gradient_checkpointing_enable()\n","model = prepare_model_for_kbit_training(model)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1698792525274,"user":{"displayName":"Shawn McInerney","userId":"16229430297181086922"},"user_tz":240},"id":"LkQmGWXvrNBp"},"outputs":[],"source":["lora_config = LoraConfig(\n","    r=16,\n","    lora_alpha=32,\n","    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\",\"o_proj\"],\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","peft_model = get_peft_model(model, lora_config)"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1698792525274,"user":{"displayName":"Shawn McInerney","userId":"16229430297181086922"},"user_tz":240},"id":"-QETeqWrTNjR"},"outputs":[],"source":["model.add_adapter(lora_config, adapter_name=\"adapter\")"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":102,"referenced_widgets":["0823e984d97541569168286eca2e5a8d","9b0eb8cf51e8409b933e5d656f82405f","d9ca206faad941f7999730df2d35bcc6","7c2c61bcf88e407898cda2cb17543c89","eacc344f099145d9ba1c4d541cbbd741","2dbdce70a3c34e01a6e3681b350a672e","de357c95ae7f4bed8a1c0d38d6a6717e","73a1d6235efa4b3689513b9c1708971d","d0461b8f89644def85a0c9744504187d","4aa6736b33914ddaaa686900b711b3a7","4f9c24144193484e847538645851f765"]},"executionInfo":{"elapsed":16599,"status":"ok","timestamp":1698792541871,"user":{"displayName":"Shawn McInerney","userId":"16229430297181086922"},"user_tz":240},"id":"mTdPmdXupyj8","outputId":"f2695502-23cb-42c5-8282-687ed1fc19d8"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0823e984d97541569168286eca2e5a8d"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["'\\nThe following warnings can be ignored. I think they are caused by the fact that the model\\nis loaded in a single GPU and the parameters are offloaded to the CPU:\\n\\nWARNING:root:Some parameters are on the meta device device because they were offloaded to the .\\nWARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu/disk.\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}],"source":["# Reload the model to avoid the following error. Don't know why, but this has to be done:\"\n","# Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! when resuming training\"\n","model = AutoModelForCausalLM.from_pretrained(pretrained_model, quantization_config=bnb_config, device_map={\"\":0})\n","\n","'''\n","The following warnings can be ignored. I think they are caused by the fact that the model\n","is loaded in a single GPU and the parameters are offloaded to the CPU:\n","\n","WARNING:root:Some parameters are on the meta device device because they were offloaded to the .\n","WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu/disk.\n","'''"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":160},"executionInfo":{"elapsed":6156,"status":"ok","timestamp":1698792548016,"user":{"displayName":"Shawn McInerney","userId":"16229430297181086922"},"user_tz":240},"id":"pQyMqLg5izHF","outputId":"81db9e14-c479-4ac3-ea63-72b64b709a08"},"outputs":[{"output_type":"stream","name":"stdout","text":["Steps per epoch: 1000\n","Total steps: 4000\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:214: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["\"\\nThe following warning can be ignored as far as I can tell. Via web search: it happens to many people,\\nand no fix has been found yet:\\n\\nUserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer.\\nThis might lead to some unexpected behaviour due to overflow issues when training a model in half-precision.\\nYou might consider adding `tokenizer.padding_side = 'right'` to your code.\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}],"source":["%pip install -q trl\n","from trl import SFTTrainer\n","\n","epochs = 4 # 4 was recommended by someone on the OpenAI forum, unless dataset is very small.\n","per_device_train_batch_size = 2\n","gradient_accumulation_steps = 4\n","max_seq_length = 512\n","\n","steps_per_epoch = len(train_data)//(per_device_train_batch_size*gradient_accumulation_steps)\n","print(\"Steps per epoch:\", steps_per_epoch)\n","\n","total_steps = steps_per_epoch * epochs\n","print(\"Total steps:\", total_steps)\n","\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\"\n","\n","torch.cuda.empty_cache()\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=train_data,\n","    eval_dataset=test_data,\n","    dataset_text_field=\"prompt\",\n","    peft_config=lora_config,\n","    max_seq_length=max_seq_length,\n","    args=TrainingArguments(\n","        per_device_train_batch_size=per_device_train_batch_size,\n","        gradient_accumulation_steps=gradient_accumulation_steps,\n","        warmup_steps=0.03,\n","        learning_rate=2e-4,\n","        logging_steps=steps_per_epoch,\n","        output_dir=output_dir,\n","        optim=\"paged_adamw_8bit\",\n","        save_strategy=\"epoch\",\n","        evaluation_strategy=\"epoch\",\n","        num_train_epochs=epochs,\n","        save_steps=steps_per_epoch,\n","        load_best_model_at_end=True,\n","        fp16=True,\n","        gradient_checkpointing=True,\n","    ),\n","    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",")\n","\n","'''\n","The following warning can be ignored as far as I can tell. Via web search: it happens to many people,\n","and no fix has been found yet:\n","\n","UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer.\n","This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision.\n","You might consider adding `tokenizer.padding_side = 'right'` to your code.\n","'''"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":430},"executionInfo":{"elapsed":7803626,"status":"ok","timestamp":1698800351639,"user":{"displayName":"Shawn McInerney","userId":"16229430297181086922"},"user_tz":240},"id":"W4HNvrh5FYqM","outputId":"67124404-c323-4868-c95e-e8aff3a3d362"},"outputs":[{"output_type":"stream","name":"stderr","text":["You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='4000' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4000/4000 2:10:00, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.505600</td>\n","      <td>0.500225</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.487700</td>\n","      <td>0.499272</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.485200</td>\n","      <td>0.499343</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.484300</td>\n","      <td>0.499461</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["\"\\nThe following warning can be ignored as far as I can tell. Via web search: it happens to many people\\nand no fix has been found yet:\\n\\nYou're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer,\\nusing the `__call__` method is faster than using a method to encode the text followed\\nby a call to the `pad` method to get a padded encoding.\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}],"source":["model.config.use_cache = False  # silence the warnings.\n","trainer.train()\n","\n","'''\n","The following warning can be ignored as far as I can tell. Via web search: it happens to many people\n","and no fix has been found yet:\n","\n","You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer,\n","using the `__call__` method is faster than using a method to encode the text followed\n","by a call to the `pad` method to get a padded encoding.\n","'''"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1698800351640,"user":{"displayName":"Shawn McInerney","userId":"16229430297181086922"},"user_tz":240},"id":"o4mkyx1dww6p"},"outputs":[],"source":["trainer.save_model(finetuned_model_folder)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5fee1338b240490c8005655b5889e095":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c39bfa793443401cb29f3d6e27d39d0a","IPY_MODEL_6764c8e69733490b84a50eb46af596ca","IPY_MODEL_956c0c8d8f8d40d2bd184e9ccff04197"],"layout":"IPY_MODEL_cf46938c0c5e415b96f865e23c930d52"}},"c39bfa793443401cb29f3d6e27d39d0a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_16453c65225344a5bd070fcb61356976","placeholder":"​","style":"IPY_MODEL_019b195413ea4d9899f572939f75df0c","value":"Loading checkpoint shards: 100%"}},"6764c8e69733490b84a50eb46af596ca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8e0d381885c43f6a3101f41e0f47fdf","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7aec764934ac45c894d58ae89d8189bf","value":2}},"956c0c8d8f8d40d2bd184e9ccff04197":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f89ee9eccd84926bb13f8413d93233f","placeholder":"​","style":"IPY_MODEL_8b4f48571db74c938603f7c3aaa8580c","value":" 2/2 [00:17&lt;00:00,  8.00s/it]"}},"cf46938c0c5e415b96f865e23c930d52":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16453c65225344a5bd070fcb61356976":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"019b195413ea4d9899f572939f75df0c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e8e0d381885c43f6a3101f41e0f47fdf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7aec764934ac45c894d58ae89d8189bf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8f89ee9eccd84926bb13f8413d93233f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b4f48571db74c938603f7c3aaa8580c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0823e984d97541569168286eca2e5a8d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9b0eb8cf51e8409b933e5d656f82405f","IPY_MODEL_d9ca206faad941f7999730df2d35bcc6","IPY_MODEL_7c2c61bcf88e407898cda2cb17543c89"],"layout":"IPY_MODEL_eacc344f099145d9ba1c4d541cbbd741"}},"9b0eb8cf51e8409b933e5d656f82405f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2dbdce70a3c34e01a6e3681b350a672e","placeholder":"​","style":"IPY_MODEL_de357c95ae7f4bed8a1c0d38d6a6717e","value":"Loading checkpoint shards: 100%"}},"d9ca206faad941f7999730df2d35bcc6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_73a1d6235efa4b3689513b9c1708971d","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d0461b8f89644def85a0c9744504187d","value":2}},"7c2c61bcf88e407898cda2cb17543c89":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4aa6736b33914ddaaa686900b711b3a7","placeholder":"​","style":"IPY_MODEL_4f9c24144193484e847538645851f765","value":" 2/2 [00:14&lt;00:00,  6.91s/it]"}},"eacc344f099145d9ba1c4d541cbbd741":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2dbdce70a3c34e01a6e3681b350a672e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de357c95ae7f4bed8a1c0d38d6a6717e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"73a1d6235efa4b3689513b9c1708971d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0461b8f89644def85a0c9744504187d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4aa6736b33914ddaaa686900b711b3a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f9c24144193484e847538645851f765":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}