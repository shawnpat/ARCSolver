Using device: (NVIDIA GeForce RTX 3080 Laptop GPU)
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
WARNING:root:Some parameters are on the meta device device because they were offloaded to the .
WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu/disk.
WARNING:root:Some parameters are on the meta device device because they were offloaded to the .
WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu/disk.
Note: you may need to restart the kernel to use updated packages.
Steps per epoch: 160
Total steps: 640
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'loss': 0.718, 'learning_rate': 0.0001500070315796053, 'epoch': 1.0}
{'eval_loss': 0.6186798810958862, 'eval_runtime': 20.5438, 'eval_samples_per_second': 1.947, 'eval_steps_per_second': 0.243, 'epoch': 1.0}
{'loss': 0.5422, 'learning_rate': 0.00010000468771973686, 'epoch': 2.0}
{'eval_loss': 0.6211872696876526, 'eval_runtime': 21.3743, 'eval_samples_per_second': 1.871, 'eval_steps_per_second': 0.234, 'epoch': 2.0}
{'loss': 0.5285, 'learning_rate': 5.000234385986843e-05, 'epoch': 3.0}
{'eval_loss': 0.6097601652145386, 'eval_runtime': 22.0263, 'eval_samples_per_second': 1.816, 'eval_steps_per_second': 0.227, 'epoch': 3.0}
{'loss': 0.5158, 'learning_rate': 0.0, 'epoch': 4.0}
{'eval_loss': 0.6153662800788879, 'eval_runtime': 26.4235, 'eval_samples_per_second': 1.514, 'eval_steps_per_second': 0.189, 'epoch': 4.0}
