
Using device: (NVIDIA GeForce RTX 3080 Laptop GPU)
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
WARNING:root:Some parameters are on the meta device device because they were offloaded to the .
WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu/disk.
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
Steps per epoch: 800
Total steps: 3200
WARNING:root:Some parameters are on the meta device device because they were offloaded to the .
WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu/disk.
/home/shawn/Projects/LLM_ARC_Solver/.venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:214: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'loss': 0.6267, 'learning_rate': 0.00015000140626318374, 'epoch': 1.0}
{'eval_loss': 0.7183597683906555, 'eval_runtime': 115.1128, 'eval_samples_per_second': 1.737, 'eval_steps_per_second': 0.217, 'epoch': 1.0}
{'loss': 0.5975, 'learning_rate': 0.00010000093750878917, 'epoch': 2.0}
{'eval_loss': 0.7340261936187744, 'eval_runtime': 113.1947, 'eval_samples_per_second': 1.767, 'eval_steps_per_second': 0.221, 'epoch': 2.0}
{'loss': 0.5821, 'learning_rate': 5.000046875439458e-05, 'epoch': 3.0}
{'eval_loss': 0.7065547704696655, 'eval_runtime': 113.6833, 'eval_samples_per_second': 1.759, 'eval_steps_per_second': 0.22, 'epoch': 3.0}
{'loss': 0.5726, 'learning_rate': 0.0, 'epoch': 4.0}
{'eval_loss': 0.7097418308258057, 'eval_runtime': 114.455, 'eval_samples_per_second': 1.747, 'eval_steps_per_second': 0.218, 'epoch': 4.0}
