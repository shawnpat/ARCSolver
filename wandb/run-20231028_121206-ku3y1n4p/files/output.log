
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
WARNING:root:Some parameters are on the meta device device because they were offloaded to the .
WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu/disk.
WARNING:root:Some parameters are on the meta device device because they were offloaded to the .
WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu/disk.
Note: you may need to restart the kernel to use updated packages.
Steps per epoch: 160
Total steps: 640
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'loss': 0.523, 'learning_rate': 0.0001500070315796053, 'epoch': 1.0}
{'eval_loss': 0.42654141783714294, 'eval_runtime': 28.0453, 'eval_samples_per_second': 1.426, 'eval_steps_per_second': 0.178, 'epoch': 1.0}
{'loss': 0.3782, 'learning_rate': 0.00010000468771973686, 'epoch': 2.0}
{'eval_loss': 0.42658621072769165, 'eval_runtime': 28.9847, 'eval_samples_per_second': 1.38, 'eval_steps_per_second': 0.173, 'epoch': 2.0}
{'loss': 0.37, 'learning_rate': 5.000234385986843e-05, 'epoch': 3.0}
{'eval_loss': 0.42407727241516113, 'eval_runtime': 29.7815, 'eval_samples_per_second': 1.343, 'eval_steps_per_second': 0.168, 'epoch': 3.0}
{'loss': 0.3654, 'learning_rate': 0.0, 'epoch': 4.0}
{'eval_loss': 0.4264252781867981, 'eval_runtime': 31.3759, 'eval_samples_per_second': 1.275, 'eval_steps_per_second': 0.159, 'epoch': 4.0}
