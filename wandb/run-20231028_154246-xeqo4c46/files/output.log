
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
WARNING:root:Some parameters are on the meta device device because they were offloaded to the .
WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu/disk.
WARNING:root:Some parameters are on the meta device device because they were offloaded to the .
WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu/disk.
Note: you may need to restart the kernel to use updated packages.
Steps per epoch: 80
Total steps: 320
/home/shawn/Projects/LLM_ARC_Solver/.venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:214: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'loss': 0.6064, 'learning_rate': 0.000150014063818483, 'epoch': 1.0}
{'eval_loss': 0.5552311539649963, 'eval_runtime': 11.6201, 'eval_samples_per_second': 1.721, 'eval_steps_per_second': 0.258, 'epoch': 1.0}
{'loss': 0.4172, 'learning_rate': 0.00010000937587898864, 'epoch': 2.0}
{'eval_loss': 0.5347091555595398, 'eval_runtime': 12.0361, 'eval_samples_per_second': 1.662, 'eval_steps_per_second': 0.249, 'epoch': 2.0}
{'loss': 0.3999, 'learning_rate': 5.000468793949432e-05, 'epoch': 3.0}
{'eval_loss': 0.5314777493476868, 'eval_runtime': 12.595, 'eval_samples_per_second': 1.588, 'eval_steps_per_second': 0.238, 'epoch': 3.0}
{'loss': 0.3767, 'learning_rate': 0.0, 'epoch': 4.0}
{'eval_loss': 0.5420790910720825, 'eval_runtime': 12.4196, 'eval_samples_per_second': 1.61, 'eval_steps_per_second': 0.242, 'epoch': 4.0}
{'train_runtime': 517.1853, 'train_samples_per_second': 0.619, 'train_steps_per_second': 0.619, 'train_loss': 0.4500486493110657, 'epoch': 4.0}
/home/shawn/Projects/LLM_ARC_Solver/.venv/lib/python3.11/site-packages/transformers/utils/hub.py:850: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
shawnmci/Mistral-7B-Instruct_100_finetuned-test
shawnmci/test
shawnmci/test
/home/shawn/Projects/LLM_ARC_Solver/.venv/lib/python3.11/site-packages/transformers/utils/hub.py:850: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
shawnmci/Mistral-7B-Instruct-v0.1-ARCSolver-arrays
shawnmci/Mistral-7B-Instruct-v0.1-ARCSolver-arrays
Mistral-7B-Instruct-v0.1-ARCSolver-arrays
