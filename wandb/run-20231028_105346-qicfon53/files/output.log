Using device: (NVIDIA GeForce RTX 3080 Laptop GPU)
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
Note: you may need to restart the kernel to use updated packages.
WARNING:root:Some parameters are on the meta device device because they were offloaded to the .
WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu/disk.
WARNING:root:Some parameters are on the meta device device because they were offloaded to the .
WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu/disk.
Note: you may need to restart the kernel to use updated packages.
Steps per epoch: 80
Total steps: 320
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'loss': 0.8476, 'learning_rate': 0.000150014063818483, 'epoch': 1.0}
{'eval_loss': 0.646352231502533, 'eval_runtime': 10.435, 'eval_samples_per_second': 1.917, 'eval_steps_per_second': 0.287, 'epoch': 1.0}
{'loss': 0.5449, 'learning_rate': 0.00010000937587898864, 'epoch': 2.0}
{'eval_loss': 0.6397145390510559, 'eval_runtime': 11.5518, 'eval_samples_per_second': 1.731, 'eval_steps_per_second': 0.26, 'epoch': 2.0}
{'loss': 0.5315, 'learning_rate': 5.000468793949432e-05, 'epoch': 3.0}
{'eval_loss': 0.6341865062713623, 'eval_runtime': 11.9017, 'eval_samples_per_second': 1.68, 'eval_steps_per_second': 0.252, 'epoch': 3.0}
{'loss': 0.5166, 'learning_rate': 0.0, 'epoch': 4.0}
{'eval_loss': 0.6335951685905457, 'eval_runtime': 11.3669, 'eval_samples_per_second': 1.759, 'eval_steps_per_second': 0.264, 'epoch': 4.0}
{'train_runtime': 504.2108, 'train_samples_per_second': 0.635, 'train_steps_per_second': 0.635, 'train_loss': 0.6101233720779419, 'epoch': 4.0}
[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.login() after wandb.init() has no effect.